## 介绍
这份手册主要是记录为什么写这个项目，以及这个项目中的一些是实现，为什么这么做？  
1、为什么要做这个项目？  
因为，我发现一篇论文的方法，可以放在这个mario上，而mario是我一开始就想完成的游戏。  
其他的mario的方法，不完整，给我感觉不像是人在玩这个游戏，我希望，agent的游玩更多的接近人的体验。  

2、这个项目的摘要
本项目，会从下面几个方面入手：
- 游戏层面，奖励函数，动作空间的修改
- 训练框架，将会采用多进程，其中N个交互进程，1个训练进程
- agent，可供选择的有很多：如PPO，SAC，TD3，DQN
- eval，完整的评估模块，记录训练中的数据，以及交互环境的数据，还有奖励的分布
- utils，放一些处理的函数   



## 正文
1、游戏层面的修改  

1.1 奖励函数的修改  
在原始的super-mario-gym库里面，奖励函数由3个部分构成：鼓励向右；时间惩罚；死亡惩罚。  
这个奖励函数，只能覆盖一直向右，且没有迷宫，没有谜题的关卡(如2-1的谜题，8-4的迷宫)  
但是我也没想到一个好的方法来引导agent去发现谜题和迷宫，这涉及一些超越游戏的先验知识，  
我能想到的最简单的方法是：引入score  
为什么引入SCORE？
因为，里面包含了与环境交互的每个步骤，比如进入隐藏地图，获得奖励；或者说在空气中打开一个谜题(隐藏砖块)，  
获得奖励；以及通关的结算，增加score。可以说游戏和score密切相关，我们对agent，如何学习，学习到了什么，  
存在一些困惑，但这确实是最直接的获得游戏信息的方法了。
我的奖励函数：
- 鼓励向右：可以用原始库的方法(可能要放大数值)
- score差值：结束帧与起始帧的差值
- 时间惩罚：可以用原始库的方法(可能要放大数值)
- 死亡惩罚:死亡将会丢失所有的score，以及一个负值惩罚
- 通关奖励:在到达斧头或者旗帜时，算作通关，保留最终的score(不保留了，还是按照差值记录)

一些trick:
- score的数值，在NES版mario中，也即经典版mario，分数可以记录的最高是：999，999；  
所以我会对score进行缩放：score = score/10000，不过对于一局来说，这个缩放有点太大了，所以改为score = score/100

实验简述：  
1、离散和连续的实验




1.2 动作空间的修改  
人是用手柄与机器交互的，按压手柄按键是一个连续的时间量，而操作传递到游戏系统，影响不同画面帧  
什么是帧？
帧只有整数计量，且以秒作为参考，如果小数，会向下或者向上取整,比如NES日本北美标准是60.0988FPS，表示60.0988帧每秒  
还有目前玩游戏看到的动态的FPS，也就是一秒钟的帧率是波动的：240，120，250....这样的序列
最常见的思考方式就是：我按压操作按钮，影响多少帧的画面  
有了以上的知识，动作空间将会由两个基础部分组成：
- 动作元素：NES可以操作游戏角色的只有——上下左右，AB，六个按键，但是在这个库里面多了一个noop
- 按压的时间：每个按键的具体量是一个连续的非负值，需不需要设置max，两个角度都有理由：  
1、需要，因为agent可能会产生一个很大的值，导致局面的崩盘，以及训练的不稳定，而对于人，按压很久也不太可能  
2、不需要，与环境交互，agent会学习到很大的值价值是很低的，从而避免这个情况  
需不需要设置min：  
1、需要，因为，很小的一个数1e-10，可以认为没有操作  
2、不需要，大于0.5的就算作1帧  
怎么做，应该需要实验  

一些trick:
- 多个动作怎么处理：  
多个动作，以动作值最小的为准，等它结束了，就是环境又一次交互  
环境中维护一个帧数组，1*action_dim的大小，然后动作值进来先换算为帧数，更新帧数组  
接着，用这个帧数组中最小的一个值，与环境交互。
- 帧数，我采用4舍5入，默认游戏系统60FPS,然后保留首尾两帧，中间随机挑选，总共N帧(超参数)  
这个N帧同时也是限定的最少的帧数(相当于离散情况的固定N帧)

一些困惑：
- 这种预先指定按多少帧，完全和人类的操作相背了(突然想到，我可以让它减啊！)
- 离散的一帧一帧的让agent去反应，似乎才对
- 不过还是这么做把，无非是视角问题，离散更精确，连续更模糊，看后续实验把！
- 在真正做实验时，发现到底要不要增加负的值，还是说直接让动作值最大的动作一直执行？  
这代表两个思路：有负值，规划了一个操作，中途放弃；没负值，就是规划了动作，就做这个动作到结束  
这两个思路最终都又概率进入，快速向右碰到小怪死去的局部解  
这两个思路经过实验，又慢效果又差，难顶！  
只规划动作的思路，在随机参数开始有一个不错的结果，但越训练越差  
1、不能训练多了，在ppoupadate为50时，和30相比，训练到相同步数，50的已经烂掉了  
2、目前采样时间还是太慢了(可能要考虑多进程了)  
目前考虑是：回放池的问题，我将会让回放池由几个小的回放池组成，每次填满小的回放池就开始训练。不过要注意，改变回放池，建立在
目前是一个单进程的环境，如果是多进程，这种方式可能就不行了。(所以我还是先往后面推进)  
总结：  
1、采样效率很低，导致训练很慢
2、训练的成果目前看，还不如不训练的初始化的状态
3、但连续动作有个潜力，他相比离散动作，发现了隐藏门和隐藏奖励
4、由于负值的情况不知道怎么处理，所以后续的连续动作采用sigmoid和(-1,1)作为边界


2、多进程  
利用多进程，同时维护多个环境，然后网络通过一个集成actor与集成环境交互，一个learner在回放池中学习。  

2.1同步  
同步，可以不用多进程，用类似EDAC的N个网络叠放到一起，开N个环境就可以(这里需要多进程吗？需要)  
然后，同步就是，采样数据，训练模型，是有一个先后顺序的

2.2异步  
异步就是，采样数据和训练模型时分开的，甚至可以同时进行，只是涉及模型的载入和写入的时候需要考虑一下  






