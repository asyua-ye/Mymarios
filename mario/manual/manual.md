## 介绍
这份手册主要是记录为什么写这个项目，以及这个项目中的一些是实现，为什么这么做？  
1、为什么要做这个项目？  
因为，我发现一篇论文的方法，可以放在这个mario上，而mario是我一开始就想完成的游戏。  
其他的mario的方法，不完整，给我感觉不像是人在玩这个游戏，我希望，agent的游玩更多的接近人的体验。  

2、这个项目的摘要
本项目，会从下面几个方面入手：
- 游戏层面，奖励函数，动作空间的修改
- 训练框架，将会采用多进程，其中N个交互进程，1个训练进程
- agent，可供选择的有很多：如PPO，SAC，TD3，DQN
- eval，完整的评估模块，记录训练中的数据，以及交互环境的数据，还有奖励的分布
- utils，放一些处理的函数   



## 正文
1、游戏层面的修改  

1.1 奖励函数的修改  
在原始的super-mario-gym库里面，奖励函数由3个部分构成：鼓励向右；时间惩罚；死亡惩罚。  
这个奖励函数，只能覆盖一直向右，且没有迷宫，没有谜题的关卡(如2-1的谜题，8-4的迷宫)  
但是我也没想到一个好的方法来引导agent去发现谜题和迷宫，这涉及一些超越游戏的先验知识，  
我能想到的最简单的方法是：引入score  
为什么引入SCORE？
因为，里面包含了与环境交互的每个步骤，比如进入隐藏地图，获得奖励；或者说在空气中打开一个谜题(隐藏砖块)，  
获得奖励；以及通关的结算，增加score。可以说游戏和score密切相关，我们对agent，如何学习，学习到了什么，  
存在一些困惑，但这确实是最直接的获得游戏信息的方法了。
我的奖励函数：
- 鼓励向右：可以用原始库的方法(可能要放大数值)
- score差值：结束帧与起始帧的差值
- 时间惩罚：可以用原始库的方法(可能要放大数值)
- 死亡惩罚:死亡将会丢失所有的score，以及一个负值惩罚
- 通关奖励:在到达斧头或者旗帜时，算作通关，保留最终的score(不保留了，还是按照差值记录)

一些trick:
- score的数值，在NES版mario中，也即经典版mario，分数可以记录的最高是：999，999；  
所以我会对score进行缩放：score = score/10000，不过对于一局来说，这个缩放有点太大了，所以改为score = score/100

实验简述：  
1、离散和连续的实验
离散的似乎，没有连续的那么强的探索，但是两者都表现得不好




1.2 动作空间的修改  
人是用手柄与机器交互的，按压手柄按键是一个连续的时间量，而操作传递到游戏系统，影响不同画面帧  
什么是帧？
帧只有整数计量，且以秒作为参考，如果小数，会向下或者向上取整,比如NES日本北美标准是60.0988FPS，表示60.0988帧每秒  
还有目前玩游戏看到的动态的FPS，也就是一秒钟的帧率是波动的：240，120，250....这样的序列
最常见的思考方式就是：我按压操作按钮，影响多少帧的画面  
有了以上的知识，动作空间将会由两个基础部分组成：
- 动作元素：NES可以操作游戏角色的只有——上下左右，AB，六个按键，但是在这个库里面多了一个noop
- 按压的时间：每个按键的具体量是一个连续的非负值，需不需要设置max，两个角度都有理由：  
1、需要，因为agent可能会产生一个很大的值，导致局面的崩盘，以及训练的不稳定，而对于人，按压很久也不太可能  
2、不需要，与环境交互，agent会学习到很大的值价值是很低的，从而避免这个情况  
需不需要设置min：  
1、需要，因为，很小的一个数1e-10，可以认为没有操作  
2、不需要，大于0.5的就算作1帧  
怎么做，应该需要实验  

一些trick:
- 多个动作怎么处理：  
多个动作，以动作值最小的为准，等它结束了，就是环境又一次交互  
环境中维护一个帧数组，1*action_dim的大小，然后动作值进来先换算为帧数，更新帧数组  
接着，用这个帧数组中最小的一个值，与环境交互。
- 帧数，我采用4舍5入，默认游戏系统60FPS,然后保留首尾两帧，中间随机挑选，总共N帧(超参数)  
这个N帧同时也是限定的最少的帧数(相当于离散情况的固定N帧)

一些困惑：
- 这种预先指定按多少帧，完全和人类的操作相背了(突然想到，我可以让它减啊！)
- 离散的一帧一帧的让agent去反应，似乎才对
- 不过还是这么做把，无非是视角问题，离散更精确，连续更模糊，看后续实验把！
- 在真正做实验时，发现到底要不要增加负的值，还是说直接让动作值最大的动作一直执行？  
这代表两个思路：有负值，规划了一个操作，中途放弃；没负值，就是规划了动作，就做这个动作到结束  
这两个思路最终都又概率进入，快速向右碰到小怪死去的局部解  
这两个思路经过实验，又慢效果又差，难顶！  
只规划动作的思路，在随机参数开始有一个不错的结果，但越训练越差  
1、不能训练多了，在ppoupadate为50时，和30相比，训练到相同步数，50的已经烂掉了  
2、目前采样时间还是太慢了(可能要考虑多进程了)  
目前考虑是：回放池的问题，我将会让回放池由几个小的回放池组成，每次填满小的回放池就开始训练。不过要注意，改变回放池，建立在
目前是一个单进程的环境，如果是多进程，这种方式可能就不行了。(所以我还是先往后面推进)  
总结：  
1、采样效率很低，导致训练很慢
2、训练的成果目前看，还不如不训练的初始化的状态
3、但连续动作有个潜力，他相比离散动作，发现了隐藏门和隐藏奖励
4、由于负值的情况不知道怎么处理，所以后续的连续动作采用sigmoid和(-1,1)作为边界


2、多进程  
利用多进程，同时维护多个环境，然后网络通过一个集成actor与集成环境交互，一个learner在回放池中学习。  

2.1同步  
同步，可以不用多进程，用类似EDAC的N个网络叠放到一起，开N个环境就可以(这里需要多进程吗？需要)  
然后，同步就是，采样数据，训练模型，是有一个先后顺序的

2.2异步  
异步就是，采样数据和训练模型时分开的，甚至可以同时进行，只是涉及模型的载入和写入的时候需要考虑一下  
我的异步模型，就是经典的生产者消费者问题，其中生产者是actor，消费者是learner  
这个不是异步梯度，而是异步采样，异步梯度留到后面写  
异步采样的矛盾，对于onpolicy的PPO：异步采样的生产者的时间大约是消费者的9倍，意味着用异步采样，可以在一次生产中多训练几次，这是它的
核心优势，但是多训练就好吗？不是，并且里面会产生一种情况，我t时刻消费者正在消费格子i，得到模型a，生产者用模型a生产，生产得到格子i+1，
但由于消费者比生产者时间短很多，它会发送多次模型更新，但是都是基于格子i下的a这个大家族(我们把基于同样数据集的模型，看成一个家族)，于是
生产者再次生产格子i+1(把a大家族得到的格子，统称为i+1大家族)，但是按照逻辑，我们想要生产者得到i+2，如果说把生产者生产的格子看成序列，可能得到：
i，i+1,i,i+2,i+1,i+3,i+2.....此时，似乎和我们想要的有差别，我们觉得onpolicy应该是更新模型，得到序列，更新模型，得到序列，这样严格+1的形式，
而异步采样，如果不加同步机制，得到这种序列，还不如直接用offpolicy。那么我要开发吗？也可以试试！反正我很闲。  
总结一下：异步采样的优势，就是多训练一段时间，多采样一段时间；对于onpocicy的ppo作用不大，对offpolicy的大家族有帮助。  

异步梯度留到后面！
异步梯度：每个进程维持一套(采样，单个agent训练)，感觉有点没意思，主要每个agent起点一样，只是在探索相同训练次数下的数据集的模型可能性。
这个思路可以有替代的方法，比如多个进程采样，集成agent



问题：  
1、直接用openai的并行框架？他这个框架，是同步的，不是异步的，但是可读性很差  
2、采样+存储，采样有一个采样池，专门负责存储采样的数据；存储池专门负责存储采样的数据(因为采样池每次采样完会全部丢弃，进入下一次采样)，
而在存储池的选择上，有两种方法，一种是直接存储采样池的数据(整个采样池的数据不切分直接拿过来)，一种是切分采样池的数据按条存储。这两种方法的区别：
前者好放入数据，只是采样时需要做额外的计算，后者不好放入数据需要做额外的计算，采样时不需要额外的计算。这两种方法本质是一样的，只是前者更直观。  
3、同步的框架搭好了，采样10240步的序列，3min左右，可能还能优化，不过训练，还是很垃圾，随机初始化的效果还挺好，但是训练了之后就不行了。
然后想到一个思路，对(state,action,reward)reward高的进行模仿学习，对(state,action,reward)reward低的进行探索，也就是降低这个action的价值，下次遇到这个state，采取其他的动作。当然我现在想的，是建立在离散动作的基础上，对于连续的，难度要高很多。  
这里我想到了对于PPO的探讨：  
PPO对它的利用部分很严格，主要体现在surr1=ratio,surr2_ratio=clamp(ratio,1-clip,1+clip)，min(surr1,surr2)，当ratio很大的时候，说明模型特别想做这个动作，
但是PPO限制了模型这样做，意味着陡峭的上升被禁止了，因为advs主导探索还是利用，ratio很大意味着分母一般比较小，此时advs的作用就可以放大，但是ratio很小，此时分母
一般很大，所以advs作用就变得缓慢了，对于分母是当时得到动作的概率，这个分母主要是起放大步数的作用  
然后对于onpolcy，这个给我感觉是目前这个训练不好的原因。  
对于损失函数，直接用连续动作组的概率是不好的，应该分散到每个动作，所以做了修改，不过还是没啥用。  
目前整个agent，还不如随机生成的参数  
然后想到我可以搭建一个课程学习，课程学习就是不同的奖励函数引导agent，奖励函数可以分等级喂给agent,没啥用  
没想到调整超参数，居然有奇效,调参，改变了agent的策略，有点像NRU  
通过调整超参数我收获了什么？降低gre到0.1-0.3，discount到08-0.6可以稳定训练，但是gre为0.2的时候，训练久了会不太稳定，他们
基本都有峰值(1万到2万)，但是整体情况一直提不起来(可能是随机采样导致的？不是)，总之没找到问题，不过值得庆幸的是，我加快了采样效率。  
修改了奖励函数后，发现了一些希望，这个训练很有趣，最终决定动作的是(mean,std)，在训练中，它有时会定住mean修改std，或者反过来。  
训练就像是无数次绕着山边的悬崖跑，中间稍有不慎就掉下来了，能达到山巅，但是只是昙花一现，没法每一次都到达山巅。  
是模型太小了，还是训练方法不对？为什么，在SAC中，mean是作为骨干的，std有点像噪声，而在我这，mean半天训练不上去...
ppo为什么均值总是训练不好，因为ppo用了normal.logprob的缘故只会接近z，而不会像SAC直接优化mean从而得到更好的Q值，ppo这种方式
很大程度都在优化std，mean仅仅靠近z，而不是价值最高的值，这就是问题所在，所以我应该转变思路，去试试SAC，DDPG等更好的方法！  
提一句，Aj老师的课太棒了，现在可以反向传播思考梯度，正向传播思考优化了！！  



3、两个路线

通过之前的实验，观察到了，PPO主要通过logprob调整mean和std，而mean更是仅仅在z的附近变化，也就是说主要是调整std，这不合理，mean的变化应该是Q网络
决定，而不是采样到的z，z的好坏谁来定？初始参数吗？所以要改变，保留ppo最主要的特征，截断ratio，去掉adv，添加Q网络作为调整mean和std的方法，logprob一项
只作为探索的工具。其实主要部分就是SAC，不过我这版本，可以在onpolicy和offpolicy之间切换，特别是异步采样的时候，可以无缝切换到DDPG，SAC等算法，所以可以
写一下！  


3.1、路线1：ppo的优化  
各种优化的加上去，变成了PPO-max



3.2、onpolicy与offpolicy的桥梁
暂时不知道，怎么做....  
突然想到，将actor和value分别应用onpolicy和offpolicy，会怎么样？比如value用上DQN的一系列技术，actor用SAC，PPO等技术。



3.3、路线2：offpolicy方法  
SAC与TD3？


3.4、中途的小结  
上面的方法，都不是不断上升的，给我感觉就是从一个坑跳到另一个坑，不是在爬山.....
在训练过程中，方差很大，相比于离散的动作，然后ppo是可以持续的训练的，而offpolcy的方法，目前很难训练。





6、一些想法  
我这个环境的状态是4*84*84，也即4张84*84的图片，每次回放池2400步，每次训练清空回放池，一共训练200次，
也就是200*2400*4张图片，总大小12.62GB，强化学习，最终输出无论是连续动作还是离散动作，最终都是给输入的图片输出一串字符，
也即打上标签，想象下，这个多夸张？而标签数量极端一点可以是200*2400。  
强化学习像不像一个自动特征提取器，通过奖励信息引导自动提取图片中的特征当作自己的状态信息  
今天看到了TAS，然后联想到GAN可以通过判断图片是否满足某个标准，这个似乎很符合离线学习以及模仿学习，大概意思：
TAS是人类玩家逐帧设计mario动作，然后通过游戏，这个逻辑很像agent的逻辑，并且这些视频就是天然的专家经验，通过GAN
可以不对视频图片进行处理，只是让agent模仿采取什么动作可以达到让判别器判断不出来的图片，这似乎是一个很好的思路！
人和目前的agent对于解决问题的思路有点区别，人是明确知道特征，并且能够在不同的图片中识别到，然后在对这些特征建立应对策略。  





