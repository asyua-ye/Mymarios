## 介绍

- 前世今生  
ppo是在trpo的基础上，修改了计算量大的二阶优化，转而直接对loss进行约束，达到比肩trpo甚至超过它的效果，
由于布置更简单且计算量更小，所以用的人更多。  
- 本文的目的  
本文主要是对ppo的复现进行一个叙述，ppo令人熟知的是它的clip，但是一个clip是不足以有一个好的ppo的。
因为它根本是一个基于Actor-Critic的框架，从环境的输出，到训练的方式，全部的细节，才是ppo，所以本文是我的实验记录，
记录在我这个项目上，能够有用的一些细节。  
正文很长，如果不看正文，这里是我在mario项目上加的优化细节：  
1、adv-norm  
2、PPG(Phasic Policy Gradient)  
3、并行的采样  
4、状态归一化  
5、奖励归一缩放  
6、tanh作为所有的激活函数  
7、极大化策略的熵  
8、梯度裁剪  
9、学习率线性递减  
10、优化器的epsilon的设置  
11、正交初始化  
12、评估模块  





## 正文


部署的细节  
1、并行的环境    
我采用的是python多进程，参考的是stablebaseline3,用原生库mp写的。多线程在python由于GIL的限制，不能达到并行效果。在linux和windows中，能够创建几千个进程和
几万个线程，但是当创建的数量超过cpu核心时，此时效率会下降，最好和cpu核心数量相同，能最好的利用cpu。  
多线程适用于需要共享大量数据或资源的应用，例如高性能计算、并行数据处理等。由于线程之间共享内存，可以更方便地共享数据。而多进程
适用于需要高隔离性或安全性的应用，例如独立的子任务处理、需要避免GIL限制的Python应用等。多进程可以避免共享数据时的竞争问题，提高安全性和稳定性。
后续可以参考ENVPOOL和isaac-ENV，前者是c++作为与线程操作的接口，后者能够调用GPU对于环境中一些计算的支持。  
然后题外话就是，对于PvP的游戏(围棋)，也可以通过并行环境来模拟，生成样本。

2、正交初始化  
正交初始化确保初始化的权重矩阵在数学上是正交的，这意味着权重矩阵的各列（或各行）彼此正交，且每个向量的范数为1。通过乘以sqrt(2)来缩放。
其中内积就是(1,2,3)*(1,2,3) = 1+4+9  
范数:(1,2,3)的范数是sqrt(1+4+9)
缩放，也叫gain:(1,2,3)*c = (c,2c,3c)
通常在最后的一层神经网络前面(CNN,MLP与特征提取有关的层)，一般采用通常，隐藏层的权重使用正交初始化，并按比例缩放sqrt(2)，偏置设置为0，
而在策略输出层的权重初始化缩放为 0.01，价值输出层的权重初始化缩放为 1。
在一些论文中，在连续控制任务，正交初始化优于默认的 Xavier 初始化。  

3、Adam 优化器的 Epsilon 参数  
默认Adam是1e-8，这里设置为1e-5

4、Adam 学习率衰减  
将学习率按照中的训练步数，逐渐衰减到0，比如3e-4衰减到0
这里借助torch的LambdaLR


5、GAE  
目的是减少方差和偏差，相比于多步仅仅考虑reward，gae还考虑了value。  
具体的数学公式如下：  
$$
\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
$$

$$
\hat{A}_t^{\text{GAE}(\gamma, \lambda)} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}
$$
当lamda为0时，此时TD(0)，当lamda不为0，value的引入，减少了方差，因为value就是神经网络统计不同状态或者动作的价值，reward仅仅是
单个序列下，增加多步的reward目的时减少偏差。  

6、小批量更新  
利用torch中的sample，可以对采样池中的样本打乱，随机挑选，并且每个样本都能被选中。
这样做的目的：减少样本顺序的影响，并且充分利用每一个样本  

7、adv的归一化  
两种思路：第一个对整个batch进行adv归一化；第二个对每次分好的minibatch(256这种)进行adv归一化。
有的说前面的好，有的说后面的好。我目前用的第一种  

8、clip  
PPO 引入了截断代理目标来稳定训练过程。具体来说，PPO 通过限制策略更新的幅度来防止过大的策略更新，避免训练过程中的不稳定性。截断代理目标函数的数学表达式如下：

$$
L^{\text{CLIP}}(\theta) = \hat{\mathbb{E}}_t \left[ \min \left( \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)} \hat{A}_t, \text{clip} \left( \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}, 1 - \epsilon, 1 + \epsilon \right) \hat{A}_t \right) \right]
$$
这个通过更简单的处理，达到了与TRPO相似的效果，并且更简单实现，所以大范围的使用，提起ppo就要想起这个。

9、价值的截断  
PPO 对价值函数的截断处理类似于 PPO 的截断代理目标。给定 \( V_{\text{targ}} = \text{returns} = \text{advantages} + \text{values} \)，PPO 通过最小化以下损失来拟合价值网络：

$$
L_V = \max \left[ (V_\theta^t - V_{\text{targ}})^2, \left( \text{clip}(V_\theta^t, V_{\theta_{t-1}} - \epsilon, V_{\theta_{t-1}} + \epsilon) - V_{\text{targ}} \right)^2 \right]
$$

这个在一些论文里，显示会损害性能或者对性能没有帮助，不过我参考的文章提到了这个，我就列出来一下，没有实现这个方法

10、总体损失和熵奖励  
总体损失计算公式为：

$$
\text{loss} = \text{policy\_loss} - \text{entropy} \times \text{entropy\_coefficient} + \text{value\_loss} \times \text{value\_coefficient}
$$
在正态分布下，通过最大化这个熵————entropy = 0.5 * (1 + np.log(2 * np.pi * sigma**2))，会增大sigma，也即std，就增大了随机性，相当于同样得到一个动作，
但要选择更随机的模型，在一些实验中表现得挺好。至于总体损失，是建立在共用一块特征提取层上，有个方法叫PPG(Phasic Policy Gradient ),actor更新时不影响特征提取层，在更新value网络时，连着一起更新特征提取层。这就和MP-DQN一样，可能分开更新更好，我暂时是一起更新的。


11、全局梯度截断  
通过torch的clip_grad_norm_直接对参数的梯度进行限制，具体做法是：将所有参数的梯度放到一个向量里面，如(1,2,3)，在求这个向量的L2范数，sqrt(1+4+9)，如果大于
0.5(自己设置的超参数)，就缩放，每个(1,2,3)*(0.5/sqrt(1+4+9)),这样就满足了。



12、评估  
记录训练过程中的一些数据(policyloss,valueloss,adv,mean,std,value等)，主要是用训练步数作为x轴。
[logger](https://github.com/snu-mllab/EDAC/blob/main/lifelong_rl/core/logging/logging.py#L73)，这个是rllab的框架的方式，有点复杂。
所以我重新写了，只要有记录功能就行。


13、分开网络还是共享网络  
这里的网络主要是指输出层之前的其他层，在简单的环境中(cartPole,LunarLander-v2)需要共享，而在离散的游戏环境中(mario,atari)也需要共享，不过联想到
PPG，是直接用value网络更新的特征提取层，actor网络，可能不具备自己更新的能力；不过在连续环境中，我试着用分散的网络，也是能够跑的，但结果不算好。


一些优化：  

有画面的环境  
1、NoopResetEnv  
这个包装器通过在重置时执行随机数量（介于 1 到 30 帧？之间）的空操作来采样初始状态。这个可以，我甚至可以在一开始加些随机的动作。

2、MaxAndSkipEnv  
更准确地说，代理在每第 k 帧而不是每帧上看到并选择动作，并且其最后一个动作在跳过的帧中重复。由于前进模拟器一步所需的计算量远小于让代理选择一个动作的计算量，这种技术允许代理在不显著增加运行时间的情况下，大致可以玩 k 倍的游戏。我们对所有游戏使用 k=4。[…] 首先，为了编码单帧，我们对正在编码的帧和前一帧中的每个像素颜色值取最大值。这对于去除在某些物体仅出现在偶数帧而其他物体仅出现在奇数帧的游戏中存在的闪烁现象是必要的，这是由 Atari 2600 能同时显示的精灵数量有限引起的。
这个有点没看明白。

3、EpisodicLifeEnv  
一条命就视为回合结束，可能会损害性能

4、FireResetEnv  
这个 wrapper 在重置时执行 FIRE 动作，适用于在发射之前保持静止的环境。这有啥用？

5、WarpFrame  
这个 wrapper 提取 210x160 像素图像的 Y 通道并将其调整为 84x84。

6、ClipRewardEnv  
这个 wrapper 根据奖励的符号将其分为 {+1, 0, -1}。这种修改奖励，可能只适合特定环境。

7、FrameStack  
这个 wrapper 堆叠最后的 m 帧，以便代理能够推断移动物体的速度和方向。预处理到最近的 m 帧并将其堆叠以生成 Q 函数的输入，其中 m=4。
就是堆叠，取不同动作的先进先出的累计4帧，作为单个状态。

8、共享还是分开  
共享的话，在训练的时候，更新actor和value，都会对特征采样层进行更新，PPG(Phasic Policy Gradient)，就是分成两个阶段更新，MP-DQN也是分成不同的输入更新。


9、像素大小限制  
将0-255限制到0-1，太大会导致KL散度爆炸。  


连续动作环境(mujoco)  
连续动作的经验可不可以借鉴到，一帧帧的图像上？  

1、正态分布  
当有mean和std时，通过正态分布N(mean,std)，其他分布会怎么样？


2、状态无关的logstd  
直接用nn.paramter创建logstd，与神经网络直接输出logstd相比，没太大差别。

3、动作概率  
算一个动作的action_logprob时，将所有子动作的logprob加起来，得到一个标量值，这种方法假设这些动作是独立的。
在社区里面，也有考虑动作是由相关性的，在我的Mario种，由于动作具有离散特点，但是强行变成连续的了，我会用MP-DQN的方式更新。
不能用MP-DQN，因为这个是对每个连续动作组进行价值估计，而actor-value，是对状态进行价值估计，这个不一样，只能在SAC这种做。

4、分开的价值策略网络  
在连续环境种，分开后的表现更好

5、动作裁剪  
主要是动作范围，可以直接将采样到的动作值超出边界的直接赋值边界值，也可以用tanh或者sigmoid来处理。tanh表现更好

6、状态归一化  
原始状态通过减去其运行均值并除以其方差来进行归一化。
如果对于一帧帧的图像，将cnn提取特征的输出进行归一化会怎么样？这里主要是对mujoco中的状态进行一个归一化。  
这里讨论下归一化：感觉是吧历史的信息融入进来，得到了一个总体的分布。  
更详细的讨论参考附录。


7、状态裁剪  
一篇论文里，将归一化后的状态裁剪到[-10,10]，发现对性能没啥帮助

8、奖励缩放  
原始奖励通过减去其运行均值并除以其方差来进行归一化。发现对性能提升很大，不过同样时在mujoco中做的实验。

9、奖励裁剪  
也是裁剪到[-10,10]，发现对性能没啥帮助

10、tanh
使用tanh作为隐藏层的激活函数  


LSTM  
LSTM与LSTMcell的区别，前者是序列进入，后者是序列中的一个最小组成单元进入


1、LSTM 层的初始化  
LSTM 层的权重以标准差为 1 进行初始化，偏置以 0 进行初始化。

2、将 LSTM 状态初始化为零  
每次新的回合开始，将隐藏状态和cell状态置为0开始

3、在回合结束时重置 LSTM 状态  
训练中途回合结束了，下次传入LSTM要从0开始

4、准备小批量的顺序展开  
每个小批量里面有很多顺序的LSTM序列组成

5、在训练过程中重建 LSTM 状态  
要保存LSTM的初始状态，但是真的只保留初始状态吗？过程中的隐藏状态不保留也可以吗？需要的


多离散空间  
以任天堂举例：方向+A+B，作为三个编码，其中各自包含(上，下，左，右，不按)(按，不按)(按，不按)，总共20种可能
比如(0,0,1)  


1、Clip Range Annealing  
clip的范围也可以组建减到0

2、并行梯度更新  
每个进程算出局部梯度后，交给全局模型进行更新

3、策略优化的提前停止  
通过引入先验知识，对模型训练进行约束，比如对kl散度的约束，不满足条件就停止训练策略或者策略和价值网络
比如action_log_probs - old_action_log_probs的结果是近似KL散度，可以用这个约束

4、无效动作掩蔽  
离散的动作种，有一些无效的动作，将其置为-inf，目的是消除梯度。如何知道某些动作无效？这也是一种先验知识。



debugg  

1、随机种子  
给所有用到随机生成的模块，固定随机种子，但是即使固定了，可能也不会完全一样，即使是单进程，比如EDAC，我的复现太失败了。

2、检查ratio是否等于 1  
在第一个 epoch 和第一个小批量更新期间，当新旧策略相同时，检查比率是否总是 1，因此比率为 1 并且没有需要剪裁的内容。如果比率不是 1，则意味着存在一个错误，程序没有重建展开过程中使用的概率分布。

3、检查 Kullback-Leibler (KL) 散度  
检查 KL 散度是否过高通常是有用的。我们一般发现 approx_kl 保持在 0.02 以下，如果 approx_kl 变得过高，通常意味着策略变化得太快，可能存在一个错误。

4、其他指标  
就像我的EDAC一样，固定了随机种子，Q值，完全不一样，目前还没找到原因。

5、经验法则：在 breakout 中获得 400 episodic return  
这个就是看结果，如果能够满足这个任务，应该复现得没问题。


工作复现  

1、枚举实现细节  
将所有复现细节记录下来。我确实没做到，其实应该看参考的代码，不过真的找不到不同的了(后面再试试把！)。

2、发布锁定的源代码  
这个，我的代码依赖很干净，就是公用的一些库，我只需要把版本号发出来就行了。也可以使用poetry 或 pipenv 来锁定依赖项，或者借助docker

3、 追踪实验  
考虑使用实验管理软件来追踪你的指标、超参数、代码等。它们可以通过节省数百小时在 matplotlib 上的工作时间和担心如何显示数据来提高你的生产力。商业解决方案（通常更成熟）包括 Weights and Biases 和 Neptune，开源解决方案包括 Aim、ClearML 和 Polyaxon。
这个我暂时自己写的评估用于跟踪。

4、采用单文件实现  
如果为了方便自己的理解，还是结构简单一些比较好，也不一定是单文件，只要结构清楚，不要太多的抽象父类，就行了。


模块化的讨论  
对于一些RL库，我们可以不管系统的其他部分，比如环境预处理，或者评估模块，我们只需要对齐输入输出，关心自己的算法就行了，如rlkit。
但是，要想弄清楚，学习整个系统，还是需要简单的搭建，单文件或者结构清楚的多文件就行了，可能会有重复代码以及重构难度的问题，这是为了
开发和便于理解的妥协。


异步PPO的讨论  
异步会让回放池有一些旧的经验，并且也不能保证比原来的PPO更好，如APPO，异步只是在采样过程中，可以继续训练这一个好处。
对于并行的环境，有一下库推荐：  
Procgen（Cobbe 等，2020）使用 C++ 实现本地矢量化环境，当设置 N=64 时（N 是环境的数量）能显著提高吞吐量。  
Envpool 使用 C++ 提供 Atari 和经典控制游戏的本地矢量化环境。  
Nvidia 的 Isaac Gym（Makoviychuk 等，2021）使用 torch 编写硬件加速的矢量化环境，允许用户轻松启动 N=4096 个环境。  
Google 的 Brax 使用 jax 编写硬件加速的矢量化环境，允许用户轻松启动 N=2048 个环境，并在几分钟内解决诸如 Ant 的机器人任务，而在 MuJoCo 中需要数小时的训练。  


未探索的方向：  
使用不同的 Atari 预处理方法（如 Machado 等人（2018）部分探索的那样）  
使用不同的连续动作分布（Beta 分布、压缩的高斯分布、具有完整协方差的高斯分布等），这可能需要一些调整  
在使用连续动作时使用状态依赖的标准差（有或没有对整个 actor 网络进行梯度回传）  
使用不同的 LSTM 初始化（全为 1 而不是 0，随机噪声，可学习参数等），使用 GRU 单元代替 LSTM  

单独优化价值网络：  
将价值网络当作DQN，把DQN中的东西放到训练价值网络上，如优先级回放。



## 参考

[ppo-details](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)  
[ppo-max](https://zhuanlan.zhihu.com/p/512327050)



## 附录

### 1、关于 BatchNorm、LayerNorm 和全样本动态均值方差的讨论

#### Batch Normalization（批归一化）
**工作原理**：
- **计算方式**：在每个 mini-batch 内，对每个特征计算均值和方差，然后进行归一化。
- **公式**：
  \[
  \hat{x}^{(i)} = \frac{x^{(i)} - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
  \]
  其中，\(\mu_B\) 和 \(\sigma_B^2\) 分别是 mini-batch 的均值和方差，\(\epsilon\) 是一个很小的数值，用于防止除零。

**优点**：
- **加速收敛**：通过减少内部协变量偏移，加速网络训练。
- **正则化效果**：在某种程度上具有正则化效果，有助于防止过拟合。

**缺点**：
- **依赖 mini-batch 大小**：对 mini-batch 大小敏感，小的 batch size 会影响其稳定性。
- **在 RNN 中效果不佳**：由于 RNN 处理的是序列数据，mini-batch 的计算在时间维度上不连续，BatchNorm 效果不如在 CNN 中显著。

**动态维护的变量**：
- **移动平均均值（moving mean）**：
  \[
  \text{moving\_mean} = \text{momentum} \times \text{moving\_mean} + (1 - \text{momentum}) \times \text{batch\_mean}
  \]
- **移动平均方差（moving variance）**：
  \[
  \text{moving\_variance} = \text{momentum} \times \text{moving\_variance} + (1 - \text{momentum}) \times \text{batch\_variance}
  \]

#### Layer Normalization（层归一化）
**工作原理**：
- **计算方式**：在每个样本的所有特征维度上进行归一化，而不是在 mini-batch 内计算。
- **公式**：
  \[
  \hat{x}^{(i)} = \frac{x^{(i)} - \mu}{\sqrt{\sigma^2 + \epsilon}}
  \]
  其中，\(\mu\) 和 \(\sigma^2\) 是单个样本所有特征的均值和方差。

**优点**：
- **对序列数据友好**：适用于 RNN 和 Transformer 等处理序列数据的网络。
- **不依赖 mini-batch 大小**：可以在单个样本上计算，适用于小 batch size。

**缺点**：
- **效果较弱**：在某些情况下，LayerNorm 的效果可能不如 BatchNorm 显著。

#### 全样本的动态均值和方差
这种方法涉及计算所有样本的全局均值和方差，通常用于数据预处理或在线更新的场景。

**工作原理**：
- **计算方式**：在整个训练数据或一个较大的滑动窗口上计算全局均值和方差，并在新数据到来时动态更新这些统计量。
- **公式**（增量更新）：
  \[
  \mu_{new} = \mu_{old} + \frac{x - \mu_{old}}{n}
  \]
  \[
  \sigma^2_{new} = \sigma^2_{old} + \frac{(x - \mu_{old}) (x - \mu_{new}) - \sigma^2_{old}}{n}
  \]

**优点**：
- **稳定性高**：全局统计量提供更稳定的归一化效果。
- **适用广泛**：适用于需要长期统计特征的场景，如强化学习中的状态归一化。

**缺点**：
- **计算开销大**：需要维护全局统计量，计算和存储开销较大。
- **适应性差**：无法快速适应数据分布的剧烈变化。

#### 总结
- **Batch Normalization**：适合 CNN 等需要在 mini-batch 上计算归一化的场景，能够加速收敛和具有正则化效果，但对小 batch size 敏感。
- **Layer Normalization**：适合 RNN 和 Transformer 等处理序列数据的网络，不依赖 mini-batch 大小，但在某些情况下效果不如 BatchNorm。
- **全样本的动态均值和方差**：适用于需要长期统计特征的场景，如强化学习中的状态归一化，提供稳定的归一化效果，但计算和存储开销较大。


### 2、实验细节的讨论
在mario平台上，我对std和mean是否公用一层网络在连接各自的输出层，做了实验。  
编码如下：
1、PPO-max+std，mean共享  
2、PPO-max+std，mean分离  
3、PPO-max+std，mean分离，但不做PPG  
4、PPO-max+std，mean分离，但不做PPG，以及不做learndacay  
5、PPO-max+std，mean共享，但不做PPG  
6、PPO-max+std，mean共享，但不做PPG，以及不做learndacay  
实验结果，看agent是否能提升平均表现，agent普遍在打转转，除了1和3有点希望。然后，3和4的对比，发现到训练后期，4的峰值还会出现，但是平均水平仍然没法提起来，
说明不做learndecay，会有性能损耗，但是由于训练时间普遍很短(2e5次训练)，所以这是一个问题！




