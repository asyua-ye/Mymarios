## 简述
这个文件主要是描述命名的由来以及各个文件的作用，以及详细介绍一些代码设计。

Nvs1：意思是N维向量表示，1个固定的时间帧数——4帧。
ppomax：用了三种稳定训练的方法，所以叫ppomax。
res18：学生模型，主要是设计用了学习已经训练好的32个教师模型。




ppomax下的文件夹及文件。

defectmodel：有缺陷的模型，主要是状态归一化没有区分训练和采样评估阶段，一直在收集统计信息，在训练次数很多的情况下，影响很小；然后checkpoint机制没有想好怎么设计。不过32个关卡都是在这个文件夹中训练的，所有我也保存了下来，不过在train文件夹中训练会更好。
train：只在训练阶段收集状态归一化的统计信息，其他阶段不再收集；checkpoint机制，将采样和评估的表现加在一起评估模型的表现，按照(通关率,采样和评估阶段的最小回合分数的和)的标准保存模型。
retrain：在train的基础上增加一个retrain机制，仅仅修改了学习率调度以及相应的加载模型的代码。


buffer文件：存储采样过程中的(state,action,value,done,等)，并且有回放池采样功能；然后还要负责GAE，adv归一化，奖励缩放等。
main文件：训练以及调整超参数。
getDataset：生成数据集，主要是为了蒸馏阶段。
getVideo：生成视频以及gif文件，测试模型通关率。
PPO：模型文件，以及生成动作，状态价值，保存模型加载模型等。
pre_env:对原始的gym-supermario环境的状态进行处理，和DQN对atari游戏的处理一样，将图片变成84*84，一个动作执行4帧并输出最后一帧，累计4帧84*84的图片作为一个状态；动作的设计；奖励函数的设计。
subproc_vec_env:多进程创建mairo环境，主要用于采样。




整个系统的宏观组成是：训练部分，评估部分，记录部分。
训练部分：主要介绍3个稳定训练的方法以及8-4的奖励函数的介绍。
1、状态归一化
主要是对CNN最后一层的输出，进行展平后的向量(如3317维就有3317个均值和方差)，每一位进行统计，通过滑动均值和方差，更新归一化每一位。

2、奖励缩放
维持一个奖励的滑动均值和方差，对每一个奖励进行缩放，缩放计算是：奖励值/标准差。

3、学习率衰减
线性衰减，根据梯度更新步数，从1.0衰减到0.1；然后retrain的阶段，会先有一个学习率线性上升，从0.1上升到1.0，再从1.0余弦衰减到0.1。

4、8-4奖励函数
让agent进入某些错误路线时会受到惩罚。


评估部分：每次训练好后，评估一定次数。

记录部分：可以看看test文件夹中的文件，主要是超参数，整个训练过程中的控制台输出，训练过程中的中间值不过只有总共训练回合数次的记录（如总共1000次训练，就只有1000行记录），以及tensorboard文件记录整个训练过程中的actorloss和valueloss(因为基本不看所以没有上传)。
