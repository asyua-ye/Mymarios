## v1
PPO  选择动作的方式，softmax


## v2
PPO  选择动作的方式，对logits的处理:  
当x小于0： e^x   
当x大于0： x+1


## v3
PPO+LSTMcell,选择动作的方式和v2一样


## v4
PPO+LSTMcell,选择动作的方式和v1一样



## 总结
都差不多，改变动作的选择方式，主要是之前用PPO+softmax，不收敛，所以这里也换了个方法  
要想攻克特定关卡，还是需要调整奖励函数，以及调整学习率  
主要涉及一些迷宫关卡，单纯的向右移动引导的奖励函数就不够用了  
我更想找到一个通用的方法，所以这里就搁置了  
